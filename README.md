# vimollama

🧠 A smart, offline code generation plugin for Vim using your local `llama3.2` model via [Ollama](https://ollama.com/).  
Inspired by [Cursor](https://www.cursor.so), this plugin brings AI completions, suggestions, and code generation — powered by your full repo context — right into Vim.

---

## ✨ Features

- ⚙️ Works alongside `clangd` for LSP diagnostics and navigation
- 🤖 Inline code completion and function generation via LLaMA
- 🧠 Full-project context awareness for better completions
- 📁 Walks your repository to gather relevant code
- 🔒 Fully offline — no APIs, no telemetry, no internet required

---

## 📦 Installation

Using [vim-plug](https://github.com/junegunn/vim-plug):

```vim
Plug 'yourname/vimollama'
````

Then in Vim:

```vim
:PlugInstall
```

---

## 🔧 Requirements

### 🖥️ System

* Unix-like OS (Linux/macOS/WSL)
* Terminal-based Vim or Neovim
* Git

### 🧰 Software

| Tool                               | Use                                |
| ---------------------------------- | ---------------------------------- |
| [Go](https://golang.org/) ≥ 1.18   | Build the backend CLI agent        |
| [Ollama](https://ollama.com/)      | Run the LLaMA model locally        |
| `llama3.2` model                   | Code completion model (via Ollama) |
| [clangd](https://clangd.llvm.org/) | LSP for C++ diagnostics/navigation |

Install the LLaMA model:

```bash
ollama pull llama3.2
```

---

## 🚀 Setup

1. **Install the plugin with Plug** (see above)

2. **Build the CLI backend (`llama-agent`)**

```bash
cd ~/.vim/plugged/vimollama/server
go build -o llama-agent main.go
mv llama-agent ~/bin/  # or any directory in your $PATH
```

3. **Ensure Ollama and the model are ready**

```bash
ollama list
ollama run llama3.2
```

4. **Ensure `clangd` is installed and used by your Vim LSP setup**
   (This plugin adds AI-based completions, not replaces clangd.)

---

## ⚡ Usage

Open any `.cpp` file and press:

```vim
\a
```

This triggers:

* Repo-wide context collection
* Prompt generation
* LLaMA code generation
* Output insertion in a new split buffer

Alternatively, run manually:

```vim
:call LlamaComplete()
```

---
## 🔑 Keybindings

| Key         | Action                             |
| ----------- | ---------------------------------- |
| `<leader>a` | Trigger AI-based inline completion |

Customize it:

```vim
nnoremap <leader>a :call LlamaComplete()<CR>

---

## 🛠 Architecture

```
        Vim (user)
           │
    ┌──────▼───────┐
    │ llama.vim    │   ← Vimscript plugin
    └──────┬───────┘
           │
    ┌──────▼───────┐
    │ llama-agent  │   ← Go CLI: context, prompt, Ollama call
    └──────┬───────┘
           │
    ┌──────▼───────┐
    │ llama3.2     │   ← Ollama local model
    └──────────────┘
```

---

## 🧠 Prompt Example (Generated by Plugin)

```text
You are a C++ expert. Complete the next few lines of code in this project:

// File: src/math/add.cpp
...

// File: include/math.h
...

// File: main.cpp
int main() {
```

---

## 🧪 Troubleshooting

| Problem                | Solution                                         |
| ---------------------- | ------------------------------------------------ |
| No output              | Run `llama-agent complete yourfile.cpp` manually |
| Ollama not responding  | Start it with `ollama serve` or `ollama run`     |
| No insertion in Vim    | Check `/tmp/llama_output.txt` for output         |
| Keybinding not working | Try `:call LlamaComplete()` manually             |

---

## 🔜 Planned Features

* [ ] Visual selection → context-aware prompt
* [ ] Multi-language support (Go, Python, Java)
* [ ] Docstring + unit test generation
* [ ] Refactor suggestions
* [ ] Inline editing and response replacement

---

## 🤝 Contributing

Pull requests welcome!

Ideas for contributions:

* Add more languages
* Improve context windowing
* Add chat/refactor modes
* Visual block-based completions

