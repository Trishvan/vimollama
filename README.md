# vimollama

ğŸ§  A smart, offline code generation plugin for Vim using your local `llama3.2` model via [Ollama](https://ollama.com/).  
Inspired by [Cursor](https://www.cursor.so), this plugin brings AI completions, suggestions, and code generation â€” powered by your full repo context â€” right into Vim.

---

## âœ¨ Features

- âš™ï¸ Works alongside `clangd` for LSP diagnostics and navigation
- ğŸ¤– Inline code completion and function generation via LLaMA
- ğŸ§  Full-project context awareness for better completions
- ğŸ“ Walks your repository to gather relevant code
- ğŸ”’ Fully offline â€” no APIs, no telemetry, no internet required

---

## ğŸ“¦ Installation

Using [vim-plug](https://github.com/junegunn/vim-plug):

```vim
Plug 'yourname/vimollama'
````

Then in Vim:

```vim
:PlugInstall
```

---

## ğŸ”§ Requirements

### ğŸ–¥ï¸ System

* Unix-like OS (Linux/macOS/WSL)
* Terminal-based Vim or Neovim
* Git

### ğŸ§° Software

| Tool                               | Use                                |
| ---------------------------------- | ---------------------------------- |
| [Go](https://golang.org/) â‰¥ 1.18   | Build the backend CLI agent        |
| [Ollama](https://ollama.com/)      | Run the LLaMA model locally        |
| `llama3.2` model                   | Code completion model (via Ollama) |
| [clangd](https://clangd.llvm.org/) | LSP for C++ diagnostics/navigation |

Install the LLaMA model:

```bash
ollama pull llama3.2
```

---

## ğŸš€ Setup

1. **Install the plugin with Plug** (see above)

2. **Build the CLI backend (`llama-agent`)**

```bash
cd ~/.vim/plugged/vimollama/server
go build -o llama-agent main.go
mv llama-agent ~/bin/  # or any directory in your $PATH
```

3. **Ensure Ollama and the model are ready**

```bash
ollama list
ollama run llama3.2
```

4. **Ensure `clangd` is installed and used by your Vim LSP setup**
   (This plugin adds AI-based completions, not replaces clangd.)

---

## âš¡ Usage

Open any `.cpp` file and press:

```vim
\a
```

This triggers:

* Repo-wide context collection
* Prompt generation
* LLaMA code generation
* Output insertion in a new split buffer

Alternatively, run manually:

```vim
:call LlamaComplete()
```

---
## ğŸ”‘ Keybindings

| Key         | Action                             |
| ----------- | ---------------------------------- |
| `<leader>a` | Trigger AI-based inline completion |

Customize it:

```vim
nnoremap <leader>a :call LlamaComplete()<CR>

---

## ğŸ›  Architecture

```
        Vim (user)
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ llama.vim    â”‚   â† Vimscript plugin
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ llama-agent  â”‚   â† Go CLI: context, prompt, Ollama call
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ llama3.2     â”‚   â† Ollama local model
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Prompt Example (Generated by Plugin)

```text
You are a C++ expert. Complete the next few lines of code in this project:

// File: src/math/add.cpp
...

// File: include/math.h
...

// File: main.cpp
int main() {
```

---

## ğŸ§ª Troubleshooting

| Problem                | Solution                                         |
| ---------------------- | ------------------------------------------------ |
| No output              | Run `llama-agent complete yourfile.cpp` manually |
| Ollama not responding  | Start it with `ollama serve` or `ollama run`     |
| No insertion in Vim    | Check `/tmp/llama_output.txt` for output         |
| Keybinding not working | Try `:call LlamaComplete()` manually             |

---

## ğŸ”œ Planned Features

* [ ] Visual selection â†’ context-aware prompt
* [ ] Multi-language support (Go, Python, Java)
* [ ] Docstring + unit test generation
* [ ] Refactor suggestions
* [ ] Inline editing and response replacement

---

## ğŸ¤ Contributing

Pull requests welcome!

Ideas for contributions:

* Add more languages
* Improve context windowing
* Add chat/refactor modes
* Visual block-based completions

